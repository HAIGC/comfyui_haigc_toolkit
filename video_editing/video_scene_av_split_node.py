"""
åˆ†é•œéŸ³è§†é¢‘è£å‰ªèŠ‚ç‚¹
æ ¹æ®åˆ†é•œä¿¡æ¯åŒæ­¥è£å‰ªè§†é¢‘å’ŒéŸ³é¢‘ç‰‡æ®µ
"""

import json
from typing import Dict, Tuple

import torch


class VideoSceneAVSplitNode:
    """
    åˆ†é•œéŸ³è§†é¢‘è£å‰ªèŠ‚ç‚¹

    åŠŸèƒ½ï¼š
    - æ ¹æ®åˆ†é•œè¯†åˆ«èŠ‚ç‚¹è¾“å‡ºçš„åœºæ™¯æ•°æ®ï¼Œè£å‰ªå¯¹åº”çš„è§†é¢‘ç‰‡æ®µ
    - åŒæ­¥è£å‰ªéŸ³é¢‘ï¼Œä¿è¯éŸ³è§†é¢‘æ—¶é•¿ä¸€è‡´
    - è¾“å‡ºç‹¬ç«‹çš„è§†é¢‘/éŸ³é¢‘å¯¹ï¼Œå¯ç”¨äºé¢„è§ˆæˆ–ä¿å­˜
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "images": ("IMAGE",),
                "audio": ("AUDIO",),
                "åœºæ™¯æ•°æ®": ("STRING",),
                "åœºæ™¯åºå·": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 100,
                    "step": 1,
                    "tooltip": "é€‰æ‹©éœ€è¦è£å‰ªçš„åœºæ™¯ï¼ˆä» 1 å¼€å§‹ï¼‰"
                }),
            },
        }

    RETURN_TYPES = ("IMAGE", "AUDIO", "STRING")
    RETURN_NAMES = ("è§†é¢‘", "éŸ³é¢‘", "åœºæ™¯ä¿¡æ¯")
    FUNCTION = "split_scene_av"
    CATEGORY = "HAIGCå·¥å…·é›†/è§†é¢‘å‰ªè¾‘"

    def split_scene_av(
        self,
        images: torch.Tensor,
        audio: Dict[str, torch.Tensor],
        åœºæ™¯æ•°æ®: str,
        åœºæ™¯åºå·: int,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], str]:
        """åŒæ­¥è£å‰ªåˆ†é•œçš„è§†é¢‘ä¸éŸ³é¢‘"""
        try:
            if images.dim() != 4:
                raise ValueError("è§†é¢‘å¼ é‡åº”ä¸º4ç»´ (å¸§, é«˜, å®½, é€šé“)")

            scene_data = json.loads(åœºæ™¯æ•°æ®)
            scene_changes = scene_data.get("scene_changes", [])
            scene_count = scene_data.get("scene_count", 0)
            frame_rate = float(scene_data.get("frame_rate", 30.0) or 30.0)
            total_frames = images.shape[0]

            if scene_count == 0 or len(scene_changes) == 0:
                raise ValueError("åœºæ™¯æ•°æ®æ— æ•ˆæˆ–æœªæ£€æµ‹åˆ°åœºæ™¯ï¼Œè¯·å…ˆè¿è¡Œåˆ†é•œè¯†åˆ«èŠ‚ç‚¹ã€‚")

            scene_idx = max(0, min(åœºæ™¯åºå· - 1, scene_count - 1))
            if scene_idx != åœºæ™¯åºå· - 1:
                print(f"[åˆ†é•œéŸ³è§†é¢‘è£å‰ª] æç¤ºï¼šåœºæ™¯åºå· {åœºæ™¯åºå·} è¶…å‡ºèŒƒå›´ï¼Œå·²è°ƒæ•´ä¸º {scene_idx + 1}")

            start_frame = scene_changes[scene_idx]
            end_frame = scene_changes[scene_idx + 1] if scene_idx + 1 < len(scene_changes) else total_frames

            start_frame = max(0, min(start_frame, total_frames - 1))
            end_frame = max(start_frame + 1, min(end_frame, total_frames))

            video_clip = images[start_frame:end_frame].clone()

            start_time = start_frame / frame_rate
            end_time = end_frame / frame_rate
            duration = end_time - start_time

            audio_clip = self._slice_audio(audio, start_time, end_time)

            info = (
                f"åœºæ™¯{scene_idx + 1}: "
                f"{start_time:.2f}s - {end_time:.2f}s "
                f"(æ—¶é•¿ {duration:.2f}s, å¸§ {start_frame}-{end_frame})"
            )

            print(f"[åˆ†é•œéŸ³è§†é¢‘è£å‰ª] {info}")
            print(f"[åˆ†é•œéŸ³è§†é¢‘è£å‰ª] è§†é¢‘ç‰‡æ®µ: {video_clip.shape}, éŸ³é¢‘ç‰‡æ®µ: {audio_clip['waveform'].shape}")

            return (video_clip, audio_clip, info)

        except Exception as exc:
            print(f"[åˆ†é•œéŸ³è§†é¢‘è£å‰ª] é”™è¯¯: {exc}")
            import traceback
            traceback.print_exc()

            dummy_video = images[:1].clone() if images.numel() > 0 else torch.zeros((1, 64, 64, 3))
            dummy_audio = {
                "waveform": torch.zeros((1, 2, 0)),
                "sample_rate": int(audio.get("sample_rate", 44100) or 44100) if isinstance(audio, dict) else 44100,
            }
            return (dummy_video, dummy_audio, f"å‡ºé”™: {exc}")

    def _slice_audio(
        self,
        audio: Dict[str, torch.Tensor],
        start_time: float,
        end_time: float,
    ) -> Dict[str, torch.Tensor]:
        """æ ¹æ®æ—¶é—´èŒƒå›´è£å‰ªéŸ³é¢‘"""
        if not isinstance(audio, dict) or "waveform" not in audio:
            return {"waveform": torch.zeros((1, 2, 0)), "sample_rate": 44100}

        waveform = audio.get("waveform")
        sample_rate = int(audio.get("sample_rate", 44100) or 44100)

        if waveform is None or waveform.numel() == 0:
            return {"waveform": torch.zeros((1, 2, 0)), "sample_rate": sample_rate}

        if waveform.dim() == 2:
            waveform = waveform.unsqueeze(0)

        total_samples = waveform.shape[-1]
        start_sample = int(round(max(0.0, start_time) * sample_rate))
        end_sample = int(round(max(start_time, end_time) * sample_rate))
        start_sample = min(start_sample, total_samples)
        end_sample = min(max(start_sample + 1, end_sample), total_samples)

        sliced = waveform[..., start_sample:end_sample].clone()

        return {
            "waveform": sliced,
            "sample_rate": sample_rate,
        }


NODE_CLASS_MAPPINGS = {
    "VideoSceneAVSplitNode": VideoSceneAVSplitNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoSceneAVSplitNode": "åˆ†é•œéŸ³è§†é¢‘è£å‰ª ğŸ§",
}

