"""
åˆ†é•œè½¬æ¥èŠ‚ç‚¹
æ ¹æ®åœºæ™¯è¯†åˆ«ç»“æœï¼Œé€‰æ‹©å¹¶è¾“å‡ºå¤šä¸ªåœºæ™¯ç‰‡æ®µ
"""

import torch
import json
from typing import Tuple, Optional

class VideoSceneSplitterNode:
    """åˆ†é•œè½¬æ¥èŠ‚ç‚¹ - å¯é€‰æ‹©è¾“å‡ºå¤šä¸ªåœºæ™¯ç‰‡æ®µ"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "images": ("IMAGE",),
                "åœºæ™¯æ•°æ®": ("STRING",),
                "è¾“å‡º1åœºæ™¯åºå·": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 100,
                    "step": 1,
                    "tooltip": "è¾“å‡ºç«¯å£1çš„åœºæ™¯åºå·ï¼ˆä»1å¼€å§‹ï¼Œ0è¡¨ç¤ºä¸è¾“å‡ºï¼‰"
                }),
                "è¾“å‡º2åœºæ™¯åºå·": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "tooltip": "è¾“å‡ºç«¯å£2çš„åœºæ™¯åºå·ï¼ˆä»1å¼€å§‹ï¼Œ0è¡¨ç¤ºä¸è¾“å‡ºï¼‰"
                }),
                "è¾“å‡º3åœºæ™¯åºå·": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "tooltip": "è¾“å‡ºç«¯å£3çš„åœºæ™¯åºå·ï¼ˆä»1å¼€å§‹ï¼Œ0è¡¨ç¤ºä¸è¾“å‡ºï¼‰"
                }),
                "è¾“å‡º4åœºæ™¯åºå·": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "tooltip": "è¾“å‡ºç«¯å£4çš„åœºæ™¯åºå·ï¼ˆä»1å¼€å§‹ï¼Œ0è¡¨ç¤ºä¸è¾“å‡ºï¼‰"
                }),
                "è¾“å‡º5åœºæ™¯åºå·": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "tooltip": "è¾“å‡ºç«¯å£5çš„åœºæ™¯åºå·ï¼ˆä»1å¼€å§‹ï¼Œ0è¡¨ç¤ºä¸è¾“å‡ºï¼‰"
                }),
            },
        }
    
    RETURN_TYPES = ("IMAGE", "IMAGE", "IMAGE", "IMAGE", "IMAGE")
    RETURN_NAMES = ("åœºæ™¯1", "åœºæ™¯2", "åœºæ™¯3", "åœºæ™¯4", "åœºæ™¯5")
    FUNCTION = "split_scenes"
    CATEGORY = "HAIGCå·¥å…·é›†/è§†é¢‘å‰ªè¾‘"
    
    def split_scenes(
        self,
        images: torch.Tensor,
        åœºæ™¯æ•°æ®: str,
        è¾“å‡º1åœºæ™¯åºå·: int,
        è¾“å‡º2åœºæ™¯åºå·: int,
        è¾“å‡º3åœºæ™¯åºå·: int,
        è¾“å‡º4åœºæ™¯åºå·: int,
        è¾“å‡º5åœºæ™¯åºå·: int
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        æ ¹æ®åœºæ™¯æ•°æ®åˆ†å‰²è§†é¢‘å¹¶è¾“å‡ºæŒ‡å®šåœºæ™¯
        
        Args:
            images: è¾“å…¥è§†é¢‘ï¼Œå½¢çŠ¶ (B, H, W, C)
            åœºæ™¯æ•°æ®: JSONæ ¼å¼çš„åœºæ™¯æ•°æ®å­—ç¬¦ä¸²
            è¾“å‡º1åœºæ™¯åºå· ~ è¾“å‡º5åœºæ™¯åºå·: è¦è¾“å‡ºçš„åœºæ™¯åºå·ï¼ˆ1å¼€å§‹ï¼Œ0è¡¨ç¤ºä¸è¾“å‡ºï¼‰
            
        Returns:
            5ä¸ªåœºæ™¯ç‰‡æ®µï¼ˆå¦‚æœåºå·ä¸º0æˆ–æ— æ•ˆï¼Œè¿”å›ç©ºè§†é¢‘ï¼‰
        """
        try:
            # è§£æåœºæ™¯æ•°æ®
            try:
                scene_data = json.loads(åœºæ™¯æ•°æ®)
                scene_changes = scene_data.get("scene_changes", [])
                scene_count = scene_data.get("scene_count", 0)
                frame_rate = scene_data.get("frame_rate", 30.0)
                total_frames = scene_data.get("total_frames", images.shape[0])
            except (json.JSONDecodeError, KeyError) as e:
                print(f"[åˆ†é•œè½¬æ¥] é”™è¯¯: æ— æ³•è§£æåœºæ™¯æ•°æ® - {str(e)}")
                # è¿”å›5ä¸ªç©ºè§†é¢‘ï¼ˆä¸è¾“å…¥è§†é¢‘å°ºå¯¸ä¸€è‡´çš„å•å¸§é»‘è‰²å›¾åƒï¼‰
                _, height, width, channels = images.shape
                empty_video = torch.zeros((1, height, width, channels), device=images.device)
                return (empty_video, empty_video, empty_video, empty_video, empty_video)
            
            batch_size = images.shape[0]
            _, height, width, channels = images.shape
            device = images.device
            
            # éªŒè¯åœºæ™¯æ•°æ®
            if scene_count == 0 or len(scene_changes) == 0:
                print(f"[åˆ†é•œè½¬æ¥] è­¦å‘Š: æœªæ£€æµ‹åˆ°åœºæ™¯ï¼Œè¿”å›ç©ºè§†é¢‘")
                empty_video = torch.zeros((1, height, width, channels), device=device)
                return (empty_video, empty_video, empty_video, empty_video, empty_video)
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°åœºæ™¯åˆ‡æ¢ç‚¹
            print(f"[åˆ†é•œè½¬æ¥] åœºæ™¯æ•°æ®è§£ææˆåŠŸ:")
            print(f"  æ€»åœºæ™¯æ•°: {scene_count}")
            print(f"  åœºæ™¯åˆ‡æ¢ç‚¹(å¸§): {scene_changes}")
            print(f"  è¾“å…¥è§†é¢‘æ€»å¸§æ•°: {batch_size}")
            print(f"  è§†é¢‘å¸§ç‡: {frame_rate}")
            
            # æ‰“å°æ¯ä¸ªåœºæ™¯çš„å¸§èŒƒå›´
            for idx in range(scene_count):
                start_frame = scene_changes[idx]
                end_frame = scene_changes[idx + 1] if idx + 1 < len(scene_changes) else batch_size
                start_time = start_frame / frame_rate
                end_time = end_frame / frame_rate
                duration = end_time - start_time
                print(f"  åœºæ™¯{idx+1}: å¸§{start_frame}-{end_frame} (æ—¶é•¿{duration:.2f}s)")
            
            # å¤„ç†æ¯ä¸ªè¾“å‡ºç«¯å£
            outputs = []
            output_indices = [
                è¾“å‡º1åœºæ™¯åºå·,
                è¾“å‡º2åœºæ™¯åºå·,
                è¾“å‡º3åœºæ™¯åºå·,
                è¾“å‡º4åœºæ™¯åºå·,
                è¾“å‡º5åœºæ™¯åºå·
            ]
            
            print(f"[åˆ†é•œè½¬æ¥] è¾“å‡ºé…ç½®: {output_indices}")
            
            for port_idx, scene_idx in enumerate(output_indices, 1):
                if scene_idx == 0:
                    # ä¸è¾“å‡ºï¼Œè¿”å›ç©ºè§†é¢‘ï¼ˆä¸è¾“å…¥è§†é¢‘å°ºå¯¸ä¸€è‡´çš„å•å¸§é»‘è‰²å›¾åƒï¼‰
                    empty_video = torch.zeros((1, height, width, channels), device=device)
                    outputs.append(empty_video)
                    continue
                
                # è½¬æ¢ä¸º0-basedç´¢å¼•
                scene_idx_0based = scene_idx - 1
                
                # éªŒè¯åœºæ™¯ç´¢å¼•èŒƒå›´
                if scene_idx_0based < 0:
                    print(f"[åˆ†é•œè½¬æ¥] è­¦å‘Š: è¾“å‡ºç«¯å£{port_idx}çš„åœºæ™¯åºå·{scene_idx}æ— æ•ˆï¼Œä½¿ç”¨åœºæ™¯1")
                    scene_idx_0based = 0
                elif scene_idx_0based >= scene_count:
                    print(f"[åˆ†é•œè½¬æ¥] è­¦å‘Š: è¾“å‡ºç«¯å£{port_idx}çš„åœºæ™¯åºå·{scene_idx}è¶…å‡ºèŒƒå›´ï¼ˆå…±{scene_count}ä¸ªåœºæ™¯ï¼‰ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªåœºæ™¯")
                    scene_idx_0based = scene_count - 1
                
                # è·å–å¯¹åº”åœºæ™¯çš„èµ·å§‹å’Œç»“æŸå¸§
                start_frame = scene_changes[scene_idx_0based]
                end_frame = scene_changes[scene_idx_0based + 1] if scene_idx_0based + 1 < len(scene_changes) else batch_size
                
                # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                start_frame = max(0, min(start_frame, batch_size - 1))
                end_frame = max(start_frame + 1, min(end_frame, batch_size))
                
                # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥å¸§èŒƒå›´
                if start_frame >= end_frame:
                    print(f"[åˆ†é•œè½¬æ¥] é”™è¯¯: è¾“å‡ºç«¯å£{port_idx}çš„åœºæ™¯{scene_idx}å¸§èŒƒå›´æ— æ•ˆ (start={start_frame}, end={end_frame})")
                    empty_video = torch.zeros((1, height, width, channels), device=device)
                    outputs.append(empty_video)
                    continue
                
                # æå–åœºæ™¯ç‰‡æ®µ
                scene_clip = images[start_frame:end_frame].clone()
                
                # è°ƒè¯•ä¿¡æ¯ï¼šéªŒè¯æå–çš„ç‰‡æ®µ
                print(f"[åˆ†é•œè½¬æ¥] è¾“å‡ºç«¯å£{port_idx}: æå–åœºæ™¯{scene_idx} -> å¸§{start_frame}-{end_frame}, ç‰‡æ®µå½¢çŠ¶: {scene_clip.shape}")
                
                if scene_clip.shape[0] == 0:
                    print(f"[åˆ†é•œè½¬æ¥] è­¦å‘Š: è¾“å‡ºç«¯å£{port_idx}çš„åœºæ™¯{scene_idx}ç‰‡æ®µä¸ºç©º")
                    empty_video = torch.zeros((1, height, width, channels), device=device)
                    outputs.append(empty_video)
                else:
                    outputs.append(scene_clip)
                    start_time = start_frame / frame_rate
                    end_time = end_frame / frame_rate
                    duration = end_time - start_time
                    print(f"[åˆ†é•œè½¬æ¥] è¾“å‡ºç«¯å£{port_idx}: åœºæ™¯{scene_idx} (å¸§{start_frame}-{end_frame}, æ—¶é•¿{duration:.2f}s, {scene_clip.shape[0]}å¸§)")
            
            return tuple(outputs)
            
        except Exception as e:
            print(f"[åˆ†é•œè½¬æ¥] é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            # è¿”å›5ä¸ªç©ºè§†é¢‘ï¼ˆä¸è¾“å…¥è§†é¢‘å°ºå¯¸ä¸€è‡´çš„å•å¸§é»‘è‰²å›¾åƒï¼‰
            try:
                _, height, width, channels = images.shape
                device = images.device
            except:
                height, width, channels = 64, 64, 3
                device = torch.device("cpu")
            empty_video = torch.zeros((1, height, width, channels), device=device)
            return (empty_video, empty_video, empty_video, empty_video, empty_video)
    
NODE_CLASS_MAPPINGS = {
    "VideoSceneSplitterNode": VideoSceneSplitterNode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoSceneSplitterNode": "åˆ†é•œè½¬æ¥ ğŸ¬"
}

